{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "#warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read   original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"Dataset_final_chronic.csv\"\n",
    "df=read_csv(data,index_col=0)\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering  with Quality measure using  cross-Validation\n",
    "we can predict the probabilities of the 6 problems in patients in the test data based on their membership in the clusters. This can be done by using the crossvalidation procedure. In each fold of the crossvalidation:\n",
    "\n",
    "a.  perform the clustering on the training part of the data set\n",
    "b.  assign patients from the test part of the data set obtained in step (a)\n",
    "c.  calculate a quality measure expressing the difference between the probabilities from the training data and the test data assigned to the same cluster in step (b)\n",
    "\n",
    "The quality measures could be the Root Mean Squared Error (RMSE) or the Mean Average Percentage Error (MAPE), for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import prince\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "for train_index, test_index in cv.split(df1):\n",
    "    X_train, X_test = df1.iloc[train_index], df1.iloc[test_index]\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RMSE_result=pd.DataFrame({})\n",
    "MAPE_result=pd.DataFrame({})\n",
    "\n",
    "km = KMeans(n_clusters=3,init='k-means++', max_iter=300, n_init=10) \n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "for train_index, test_index in cv.split(df1):\n",
    "    X_train, X_test = df1.iloc[train_index], df1.iloc[test_index]\n",
    "    #km.fit(X_train)\n",
    "    #y_pred_train=km.predict(X_train)\n",
    "    #y_pred_test=km.predict(X_test)\n",
    "    X_train_df=pd.DataFrame(X_train)\n",
    "    X_test_df=pd.DataFrame(X_test)\n",
    "    print(X_train.shape)\n",
    "    #print(X_train.tail())\n",
    "    # Take only 58 features for both testing and training\n",
    "    df_58_train= X_train_df[['sex','bmi','diastolic','sbp', 'hypertesion','diabetes','fatty ']]\n",
    "    df_58_test= X_test_df[['sex','bmi','diastolic','sbp', 'hypertesion','diabetes','fatty ']]\n",
    "    \n",
    "    #  Apply MCA  for both trianing and testing \n",
    "    ca_train = prince.CA(n_components=2, n_iter=3,copy=True,check_input=True,engine='auto',random_state=3)\n",
    "    ca_train= ca_train.fit(df_58_train)\n",
    "    PC_train=ca_train.row_coordinates(df_58_train)\n",
    "    PC_train.rename(columns={0:'Dim1',1:'Dim2',2:'Dim3'}, inplace=True)\n",
    "    \n",
    "    ca_test = prince.CA(n_components=2,n_iter=3,copy=True,check_input=True,engine='auto',random_state=42)\n",
    "    ca_test = ca_test.fit(df_58_test)\n",
    "    PC_test=ca_test.row_coordinates(df_58_test)\n",
    "    PC_test.rename(columns={0:'Dim1',1:'Dim2',2:'Dim3'}, inplace=True)\n",
    "    #print(PC_test.head())\n",
    "    \n",
    "    ## Normalize data  only training  before  model fitting  \n",
    "    x = PC_train.values #returns a numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    PC_train_nor = pd.DataFrame(x_scaled)\n",
    "    \n",
    "    # model fitting  and predcitons \n",
    "    km.fit(PC_train_nor)\n",
    "    y_pred_train=km.predict(PC_train)\n",
    "    y_pred_test=km.predict(PC_test)\n",
    "    #print (y_pred_train)\n",
    "    #print (y_pred_test)\n",
    "    \n",
    "     # Calculate probabilities for  train\n",
    "    X_train['cluster_train']=y_pred_train\n",
    "    df_5_train=X_train[['sex','bmi','diastolic','sbp', 'hypertesion','diabetes','fatty ','cluster_train']]\n",
    "    df_5_train.rename(columns={'sex','bmi','diastolic','sbp', 'hypertesion','diabetes','fatty ','cluster_train':'cl_train'}, inplace=True)\n",
    "    df_sum_train=df_5_train.groupby(['cl_train']).sum()\n",
    "    #col_sum_train=df_sum_train[df_sum_train.columns].sum(axis=0)\n",
    "    #df_per_train=df_sum_train/986052 # calcualte the percentage of 1's in  each  cluster \n",
    "    cls_train=pd.DataFrame(y_pred_train)\n",
    "    #cls.columns=['cluster_train']\n",
    "    gp_train=cls_train.groupby(0).size()\n",
    "    \n",
    "    c0=df_sum_train.iloc[0]/gp_train.iloc[0]\n",
    "    c1=df_sum_train.iloc[1]/gp_train.iloc[1]\n",
    "    c2=df_sum_train.iloc[2]/gp_train.iloc[2]\n",
    "    \n",
    "    df_per_train=pd.concat([c0,c1,c2],axis=1,ignore_index=True)\n",
    "    df_per_train=df_per_train.T\n",
    "        \n",
    "    # Calculate probabilities for  test\n",
    "    X_test['cluster_test']=y_pred_test\n",
    "    df_5_test=X_test[['sex','bmi','diastolic','sbp', 'hypertesion','diabetes','fatty ','cluster_test']]\n",
    "    df_5_test.rename(columns={'sex','bmi','diastolic','sbp', 'hypertesion','diabetes','fatty ','cluster_test':'cl_test'}, inplace=True)\n",
    "    df_sum_test=df_5_test.groupby(['cl_test']).sum()\n",
    "    #col_sum_test=df_sum_test[df_sum_test.columns].sum(axis=0)\n",
    "    #df_per_test=df_sum_test/109561 # calcualte the probabilities  of  outcomes  in  each  cluster \n",
    "    \n",
    "    cls_test=pd.DataFrame(y_pred_test)\n",
    "    #cls_test.columns=['cluster_test']\n",
    "    gp_test=cls_test.groupby(0).size()\n",
    "    \n",
    "    c00=df_sum_test.iloc[0]/gp_test.iloc[0]\n",
    "    c11=df_sum_test.iloc[1]/gp_test.iloc[1]\n",
    "    c22=df_sum_test.iloc[2]/gp_test.iloc[2]\n",
    "    \n",
    "    df_per_test=pd.concat([c00,c11,c22],axis=1,ignore_index=True)\n",
    "    df_per_test=df_per_test.T\n",
    "    \n",
    "    # Quality Measure  between training and testing\n",
    "    for i in range (3):\n",
    "        sum1=0\n",
    "        #print('cluster:',i,'......................................')\n",
    "        mse_sum=0\n",
    "        mape_sum=0\n",
    "        for k in range (6):\n",
    "           \n",
    "            #print(df_per_test)\n",
    "            # calcualte  Mean squared error between training and testing \n",
    "            mse=(df_per_train.iloc[i,k] - df_per_test.iloc[i,k])**2\n",
    "            mape=abs(df_per_train.iloc[i,k] - df_per_test.iloc[i,k])/abs(df_per_train.iloc[i,k])\n",
    "            #print(df_per_test.columns[k])\n",
    "            #print(df_per_train.columns[k],rms)\n",
    "            #print(df_per_train.columns[k] ,':' ,mse.round(5))\n",
    "            mse_sum=mse_sum+mse\n",
    "        MSE=mse_sum/6  #6 is teh number of problems /columns \n",
    "        RMSE=sqrt(MSE)\n",
    "                        \n",
    "        mape_sum=mape_sum+mape\n",
    "        MAPE=(mape_sum/6)*100\n",
    "            \n",
    "        RMSE_avg=pd.DataFrame([RMSE])\n",
    "        RMSE_result=RMSE_result.append(RMSE_avg) \n",
    "        \n",
    "        MAPE_avg=pd.DataFrame([MAPE])\n",
    "        MAPE_result=MAPE_result.append(MAPE_avg)\n",
    "        #print('mse_sum:',np.round(mse_sum,5))\n",
    "        # print('mape_sum:',np.round(mape_sum,5))\n",
    "        #print('RMSE:',np.round(RMSE,4))\n",
    "        #print('MAPE:',np.round(MAPE,2),'%')\n",
    "        \n",
    "        # RMSE,  over all partitions \n",
    "        #RMSE_total=0\n",
    "        #RMSE_total=RMSE_total+RMSE\n",
    "        #RMSE_avg=RMSE_total/3   # 3 is the number of  clsuters\n",
    "        \n",
    "       \n",
    "        #print('RMSE_avg:',RMSE_avg)\n",
    "        \n",
    "        #RMSE_avg=pd.DataFrame([RMSE_avg])\n",
    "        #RMSE_result=RMSE_result.append(RMSE_avg)\n",
    "        \n",
    "      \n",
    "      \n",
    "        #MAPE_total=0\n",
    "        #MAPE_total=MAPE_total+MAPE\n",
    "        #MAPE_avg=MAPE_total/3 # # 3 is the number of  clusters\n",
    "        \n",
    "       \n",
    "        #print('MAPE_avg:',MAPE_avg) \n",
    "        \n",
    "        #MAPE_avg=pd.DataFrame([MAPE_avg])\n",
    "        #MAPE_result=MAPE_result.append(MAPE_avg)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(RMSE_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('..........................................')\n",
    "rmse1=RMSE_result.iloc[0:3] \n",
    "rmse1.columns = [\"RMSE1\"]\n",
    "\n",
    "rmse2=RMSE_result.iloc[3:6]\n",
    "rmse2.columns = [\"RMSE2\"]\n",
    "\n",
    "rmse3=RMSE_result.iloc[6:9]\n",
    "rmse3.columns = [\"RMSE3\"]\n",
    "\n",
    "rmse4=RMSE_result.iloc[9:12] \n",
    "rmse4.columns = [\"RMSE4\"]\n",
    "\n",
    "rmse5=RMSE_result.iloc[12:15]\n",
    "rmse5.columns = [\"RMSE5\"]\n",
    "\n",
    "rmse6=RMSE_result.iloc[15:18]\n",
    "rmse6.columns = [\"RMSE6\"]\n",
    "\n",
    "rmse7=RMSE_result.iloc[18:21] \n",
    "rmse7.columns = [\"RMSE7\"]\n",
    "\n",
    "rmse8=RMSE_result.iloc[21:24]\n",
    "rmse8.columns = [\"RMSE8\"]\n",
    "\n",
    "rmse9=RMSE_result.iloc[24:27]\n",
    "rmse9.columns = [\"RMSE9\"]\n",
    "\n",
    "rmse10=RMSE_result.iloc[27:30]\n",
    "rmse10.columns = [\"RMSE10\"]\n",
    "\n",
    "#df_add = pd.concat([rmse1+rmse2+rmse3]+\n",
    "rmse_add=pd.concat([rmse1,rmse2,rmse3,rmse4,rmse5,rmse6,rmse7,rmse8,rmse9,rmse10],axis=1, ignore_index=True)\n",
    "rmse_add['RMSE_avg']=((rmse1['RMSE1']+rmse2['RMSE2']+rmse3['RMSE3']+rmse4['RMSE4']+\n",
    "                      rmse5['RMSE5'] +rmse6['RMSE6']+rmse7['RMSE7']+rmse8['RMSE8']+\n",
    "                      rmse9['RMSE9']+rmse10['RMSE10'])/10).round(5)\n",
    "print(rmse_add.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute percent error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(MAPE_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mape1=MAPE_result.iloc[0:3] \n",
    "mape1.columns = [\"MAPE1\"]\n",
    "\n",
    "mape2=MAPE_result.iloc[3:6]\n",
    "mape2.columns = [\"MAPE2\"]\n",
    "\n",
    "mape3=MAPE_result.iloc[6:9]\n",
    "mape3.columns = [\"MAPE3\"]\n",
    "\n",
    "mape4=MAPE_result.iloc[9:12] \n",
    "mape4.columns = [\"MAPE4\"]\n",
    "\n",
    "mape5=MAPE_result.iloc[12:15]\n",
    "mape5.columns = [\"MAPE5\"]\n",
    "\n",
    "mape6=MAPE_result.iloc[15:18]\n",
    "mape6.columns = [\"MAPE6\"]\n",
    "\n",
    "mape7=MAPE_result.iloc[18:21] \n",
    "mape7.columns = [\"MAPE7\"]\n",
    "\n",
    "mape8=MAPE_result.iloc[21:24]\n",
    "mape8.columns = [\"MAPE8\"]\n",
    "\n",
    "mape9=MAPE_result.iloc[24:27]\n",
    "mape9.columns = [\"MAPE9\"]\n",
    "mape10=MAPE_result.iloc[27:30]\n",
    "mape10.columns = [\"MAPE10\"]\n",
    "\n",
    "#df_add = pd.concat([rmse1+rmse2+rmse3]+\n",
    "mape_add=pd.concat([mape1,mape2,mape3,mape4,mape5,mape6,mape7,mape8,mape9,mape10],axis=1, ignore_index=True)\n",
    "mape_add['MAPE_avg']=((mape1['MAPE1']+mape2['MAPE2']+mape3['MAPE3']+mape4['MAPE4']+mape5['MAPE5']+\n",
    "                      mape6['MAPE6']+mape7['MAPE7']+mape8['MAPE8']+mape9['MAPE9']+mape10['MAPE10'])/10).round(2)\n",
    "print(mape_add.reset_index())\n",
    "print('..........................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
